<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Theory Seminar: Abhishek Shetty | Penn CS Theory Group</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="The speaker for this week&rsquo;s theory seminar is Abhishek Shetty from UC Berkeley. The talk will take place on Friday 4/21 at 1PM in Levine 307.
Title: Optimal PAC Bounds without Uniform Convergence
Abstract: In statistical learning theory, determining the sample complexity of realizable binary classification for VC classes was a long-standing open problem. The results of Simon and Hanneke established sharp upper bounds in this setting. However, the reliance of their argument on the uniform convergence principle limits its applicability to more general learning settings such as multiclass classification."><meta name=generator content="Hugo 0.117.0"><meta name=robots content="index, follow"><link rel=stylesheet href=/ananke/css/main.min.20b8a2d86dc4fda0308929d3b2e26e2bd0dd4405359d6ffed7a13f21487dfbc2.css><link rel="shortcut icon" href=/images/favicon.png type=image/x-icon><meta property="og:title" content="Theory Seminar: Abhishek Shetty"><meta property="og:description" content="The speaker for this week&rsquo;s theory seminar is Abhishek Shetty from UC Berkeley. The talk will take place on Friday 4/21 at 1PM in Levine 307.
Title: Optimal PAC Bounds without Uniform Convergence
Abstract: In statistical learning theory, determining the sample complexity of realizable binary classification for VC classes was a long-standing open problem. The results of Simon and Hanneke established sharp upper bounds in this setting. However, the reliance of their argument on the uniform convergence principle limits its applicability to more general learning settings such as multiclass classification."><meta property="og:type" content="article"><meta property="og:url" content="https://theory.cis.upenn.edu/post/abhishekshetty/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-04-21T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-21T00:00:00+00:00"><meta itemprop=name content="Theory Seminar: Abhishek Shetty"><meta itemprop=description content="The speaker for this week&rsquo;s theory seminar is Abhishek Shetty from UC Berkeley. The talk will take place on Friday 4/21 at 1PM in Levine 307.
Title: Optimal PAC Bounds without Uniform Convergence
Abstract: In statistical learning theory, determining the sample complexity of realizable binary classification for VC classes was a long-standing open problem. The results of Simon and Hanneke established sharp upper bounds in this setting. However, the reliance of their argument on the uniform convergence principle limits its applicability to more general learning settings such as multiclass classification."><meta itemprop=datePublished content="2023-04-21T00:00:00+00:00"><meta itemprop=dateModified content="2023-04-21T00:00:00+00:00"><meta itemprop=wordCount content="251"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Theory Seminar: Abhishek Shetty"><meta name=twitter:description content="The speaker for this week&rsquo;s theory seminar is Abhishek Shetty from UC Berkeley. The talk will take place on Friday 4/21 at 1PM in Levine 307.
Title: Optimal PAC Bounds without Uniform Convergence
Abstract: In statistical learning theory, determining the sample complexity of realizable binary classification for VC classes was a long-standing open problem. The results of Simon and Hanneke established sharp upper bounds in this setting. However, the reliance of their argument on the uniform convergence principle limits its applicability to more general learning settings such as multiclass classification."></head><body class="ma0 avenir bg-near-white production"><header class="cover bg-top" style=background-image:url(https://theory.cis.upenn.edu/images/welcome.jpg)><div class=bg-black-60><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Penn CS Theory Group</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/people/ title="People page">People</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/seminar/ title="Theory Seminar Schedule page">Theory Seminar Schedule</a></li></ul><div class=ananke-socials></div></div></div></nav><div class="tc-l pv6 ph3 ph4-ns"><div class="f2 f1-l fw2 white-90 mb0 lh-title">Theory Seminar: Abhishek Shetty</div></div></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">EVENTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Theory Seminar: Abhishek Shetty</h1><time class="f6 mv4 dib tracked" datetime=2023-04-21T00:00:00Z>April 21, 2023</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>The speaker for this week&rsquo;s theory seminar is <a href=https://ashettyv.github.io/>Abhishek Shetty</a> from UC Berkeley. The talk will take place on Friday 4/21 at 1PM in Levine 307.</p><p><strong>Title:</strong> Optimal PAC Bounds without Uniform Convergence</p><p><strong>Abstract:</strong> In statistical learning theory, determining the sample complexity of realizable binary classification for VC classes was a long-standing open problem. The results of Simon and Hanneke established sharp upper bounds in this setting. However, the reliance of their argument on the uniform convergence principle limits its applicability to more general learning settings such as multiclass classification. In this talk, we will discuss a simple technique that addresses this issue. We will present optimal high probability risk bounds through a framework that surpasses the limitations of uniform convergence arguments.</p><p>In addition to binary classification, we will see applications in three settings where uniform convergence is provably sub-optimal. For multiclass classification, we prove an optimal risk bound scaling with the one-inclusion hypergraph density of the class, addressing the suboptimality of the analysis by Daniely and Shalev-Shwartz. In partial concept classification, we determine the optimal sample complexity bound, resolving a question posed by Alon, Hanneke, Holzman, and Moran. In the context of realizable bounded regression with absolute loss, we derive an optimal risk bound that relies on a modified version of the scale-sensitive dimension, refining the results of Bartlett and Long. Our rates surpass standard uniform convergence-based results due to the smaller complexity measure in our risk bound.</p><p>Based on joint work with Ishaq Aden-Ali, Yeshwanth Cherapanamjeri and Nikita Zhivotivsky</p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><div><div class=ananke-socials></div></div></div></footer></body></html>